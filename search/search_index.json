{"config":{"lang":["en","vi"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#vietnam-zone","title":"VietNam Zone","text":"<p>Database \u0111\u01a1n v\u1ecb h\u00e0nh ch\u00ednh c\u1ee7a Vi\u1ec7t Nam</p> <p>D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u1ea5y tr\u1ef1c ti\u1ebfp t\u1eeb T\u1ed5ng C\u1ee5c Th\u1ed1ng K\u00ea Vi\u1ec7t Nam.</p> <p>C\u1eadp nh\u1eadt l\u1ea7n cu\u1ed1i: 11/04/2022</p>"},{"location":"#1-cai-at","title":"1. C\u00e0i \u0111\u1eb7t","text":""},{"location":"#11-cai-at-goi-bang-composer","title":"1.1 C\u00e0i \u0111\u1eb7t g\u00f3i b\u1eb1ng composer","text":"<pre><code>composer require kjmtrue/vietnam-zone\n</code></pre>"},{"location":"#12-copy-file-migration","title":"1.2 Copy file migration","text":"<pre><code>php artisan vendor:publish --provider=\"Kjmtrue\\VietnamZone\\ServiceProvider\"\n</code></pre>"},{"location":"#13-chinh-sua-file-migration-neu-can","title":"1.3 Ch\u1ec9nh s\u1eeda file migration n\u1ebfu c\u1ea7n","text":"<p>M\u1edf c\u00e1c file migration sau v\u00e0 tu\u1ef3 ch\u1ec9nh theo y\u00eau c\u1ea7u ri\u00eang c\u1ee7a b\u1ea1n.</p> <pre><code>database/migrations/2020_01_01_000001_create_provinces_table.php\ndatabase/migrations/2020_01_01_000002_create_districts_table.php\ndatabase/migrations/2020_01_01_000003_create_wards_table.php\n</code></pre>"},{"location":"#2-chay-migration","title":"2. Ch\u1ea1y migration","text":"<pre><code>php artisan migrate\n</code></pre>"},{"location":"#3-import-du-lieu","title":"3. Import d\u1eef li\u1ec7u","text":"<pre><code>php artisan vietnamzone:import\n</code></pre> <p>L\u01b0u \u00fd:  - D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c c\u1eadp nh\u1eadt l\u1ea7n cu\u1ed1i: 11/04/2022 - \u0110\u1ec3 c\u1eadp nh\u1eadt d\u1eef li\u1ec7u m\u1edbi nh\u1ea5t, vui l\u00f2ng l\u00e0m theo h\u01b0\u1edbng d\u1eabn \u1edf m\u1ee5c 5 tr\u01b0\u1edbc khi ch\u1ea1y l\u1ec7nh <code>php artisan vietnamzone:import</code></p>"},{"location":"#4-su-dung","title":"4. S\u1eed d\u1ee5ng","text":"<pre><code>$provinces = \\Kjmtrue\\VietnamZone\\Models\\Province::get();\n$districts = \\Kjmtrue\\VietnamZone\\Models\\District::whereProvinceId(50)-&gt;get();\n$wards = \\Kjmtrue\\VietnamZone\\Models\\Ward::whereDistrictId(552)-&gt;get();\n</code></pre>"},{"location":"#5-tai-file-du-lieu","title":"5. T\u1ea3i file d\u1eef li\u1ec7u","text":"<p>D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb T\u1ed5ng C\u1ee5c Th\u1ed1ng K\u00ea Vi\u1ec7t Nam.</p> <p>Trong t\u01b0\u01a1ng lai, khi c\u01a1 quan c\u00f3 th\u1ea9m quy\u1ec1n s\u1eafp x\u1ebfp l\u1ea1i c\u00e1c \u0111\u01a1n v\u1ecb h\u00e0nh ch\u00ednh th\u00ec b\u1ea1n c\u1ea7n ph\u1ea3i t\u1ea3i file d\u1eef li\u1ec7u m\u1edbi nh\u1ea5t tr\u01b0\u1edbc khi import d\u1eef li\u1ec7u v\u00e0o d\u1ef1 \u00e1n c\u1ee7a b\u1ea1n.</p> <p>B\u1ea1n vui l\u00f2ng l\u00e0m theo c\u00e1c b\u01b0\u1edbc h\u01b0\u1edbng d\u1eabn d\u01b0\u1edbi \u0111\u00e2y:</p> <ul> <li>Truy c\u1eadp: https://danhmuchanhchinh.gso.gov.vn/ (URL n\u00e0y c\u00f3 th\u1ec3 b\u1ecb GSOVN thay \u0111\u1ed5i)</li> <li>T\u00ecm n\u00fat \"Xu\u1ea5t Excel\"</li> <li>Tick v\u00e0o \u00f4 checkbox \"Qu\u1eadn Huy\u1ec7n Ph\u01b0\u1eddng X\u00e3\"</li> <li>Click v\u00e0o n\u00fat \"Xu\u1ea5t Excel\", v\u00e0 t\u1ea3i file xls v\u1ec1</li> <li>\u0110\u1ed5i t\u00ean file v\u1eeba t\u1ea3i v\u1ec1 th\u00e0nh <code>vnzone.xls</code> v\u00e0 copy v\u00e0o th\u01b0 m\u1ee5c <code>storage</code> c\u1ee7a d\u1ef1 \u00e1n</li> <li>Ch\u1ea1y l\u1ec7nh <code>php artisan vietnamzone:import</code> \u1edf b\u01b0\u1edbc 3</li> </ul>"},{"location":"#todo","title":"Todo","text":"<ul> <li>[ ] C\u1eadp nh\u1eadt d\u1eef li\u1ec7u</li> <li>[ ] Download file tr\u1ef1c ti\u1ebfp t\u1eeb website t\u1ed5ng c\u1ee5c th\u1ed1ng k\u00ea</li> </ul>"},{"location":"#screenshot","title":"Screenshot","text":"<p><code>select * from provinces</code></p> <pre><code>+----+------------------------+--------+---------------------+---------------------+\n| id | name                   | gso_id | created_at          | updated_at          |\n+----+------------------------+--------+---------------------+---------------------+\n|  1 | Th\u00e0nh ph\u1ed1 H\u00e0 N\u1ed9i       | 01     | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n|  2 | T\u1ec9nh H\u00e0 Giang          | 02     | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n|  3 | T\u1ec9nh Cao B\u1eb1ng          | 04     | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n|  4 | T\u1ec9nh B\u1eafc K\u1ea1n           | 06     | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n|  5 | T\u1ec9nh Tuy\u00ean Quang       | 08     | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n+----+------------------------+--------+---------------------+---------------------+\n</code></pre> <p><code>select * from districts</code></p> <pre><code>+----+-------------------+--------+-------------+---------------------+---------------------+\n| id | name              | gso_id | province_id | created_at          | updated_at          |\n+----+-------------------+--------+-------------+---------------------+---------------------+\n|  1 | Qu\u1eadn Ba \u0110\u00ecnh      | 001    |           1 | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n|  2 | Qu\u1eadn Ho\u00e0n Ki\u1ebfm    | 002    |           1 | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n|  3 | Qu\u1eadn T\u00e2y H\u1ed3       | 003    |           1 | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n|  4 | Qu\u1eadn Long Bi\u00ean    | 004    |           1 | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n|  5 | Qu\u1eadn C\u1ea7u Gi\u1ea5y     | 005    |           1 | 2020-06-16 17:22:30 | 2020-06-16 17:22:30 |\n+----+-------------------+--------+-------------+---------------------+---------------------+\n</code></pre> <p><code>select * from wards</code></p> <pre><code>+----+--------------------------+--------+-------------+---------------------+---------------------+\n| id | name                     | gso_id | district_id | created_at          | updated_at          |\n+----+--------------------------+--------+-------------+---------------------+---------------------+\n|  1 | Ph\u01b0\u1eddng Ph\u00fac X\u00e1           | 00001  |           1 | 2020-06-16 17:30:13 | 2020-06-16 17:30:13 |\n|  2 | Ph\u01b0\u1eddng Tr\u00fac B\u1ea1ch         | 00004  |           1 | 2020-06-16 17:30:13 | 2020-06-16 17:30:13 |\n|  3 | Ph\u01b0\u1eddng V\u0129nh Ph\u00fac         | 00006  |           1 | 2020-06-16 17:30:13 | 2020-06-16 17:30:13 |\n|  4 | Ph\u01b0\u1eddng C\u1ed1ng V\u1ecb           | 00007  |           1 | 2020-06-16 17:30:13 | 2020-06-16 17:30:13 |\n|  5 | Ph\u01b0\u1eddng Li\u1ec5u Giai         | 00008  |           1 | 2020-06-16 17:30:13 | 2020-06-16 17:30:13 |\n+----+--------------------------+--------+-------------+---------------------+---------------------+\n</code></pre>"},{"location":"api/classification/","title":"<code>from hulearn.classification import *</code>","text":"<p>::: hulearn.classification.functionclassifier</p> <p>::: hulearn.classification.interactiveclassifier</p>"},{"location":"api/common/","title":"<code>from hulearn.common import *</code>","text":"<p>::: hulearn.common</p>"},{"location":"api/datasets/","title":"<code>from hulearn.datasets import *</code>","text":"<p>::: hulearn.datasets</p>"},{"location":"api/interactive-charts/","title":"<code>InteractiveCharts</code>","text":"<p>::: hulearn.experimental.InteractiveCharts</p>"},{"location":"api/interactive-charts/#parallel_coordinates","title":"<code>parallel_coordinates</code>","text":"<p>::: hulearn.experimental.parallel_coordinates</p>"},{"location":"api/outlier/","title":"<code>from hulearn.outlier import *</code>","text":"<p>::: hulearn.outlier.functionoutlier</p> <p>::: hulearn.outlier.interactiveoutlier</p>"},{"location":"api/preprocessing/","title":"<code>from hulearn.preprocessing import *</code>","text":"<p>::: hulearn.preprocessing.pipetransformer</p> <p>::: hulearn.preprocessing.interactivepreprocessor</p>"},{"location":"api/regression/","title":"<code>from hulearn.regression import *</code>","text":"<p>::: hulearn.regression.functionregressor</p>"},{"location":"api/rulers/","title":"<code>CaseWhenRuler</code>","text":"<p>::: hulearn.experimental.CaseWhenRuler</p>"},{"location":"examples/faq/","title":"Frequently Asked Questions","text":"<p>Feel free to ask questions here.</p>"},{"location":"examples/faq/#what-are-the-lessons-learned","title":"What are the Lessons Learned","text":"<p>If you're interested in some of the lessons the creators of this tool learned while creating it, all you need to do is follow the python tradition.</p> <pre><code>from hulearn import this\n</code></pre>"},{"location":"examples/faq/#why-make-this","title":"Why Make This?","text":"<p>Back in the old days, it was common to write rule-based systems. Systems that do;</p> <p></p> <p>Nowadays, it's much more fashionable to use machine learning instead. Something like;</p> <p></p> <p>We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground. But we've reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too.</p> <p>This made us wonder if we could make machine learning listen more to common sense. There's a lot of things that could go wrong otherwise. If you're interested in examples we might recommend this pydata talk.</p>"},{"location":"examples/faq/#im-getting-a-port-error","title":"I'm getting a PORT error!","text":"<p>You might get an error that looks like;</p> <pre><code>ERROR:bokeh.server.views.ws:Refusing websocket connection from Origin 'http://localhost:8889';\nuse --allow-websocket-origin=localhost:8889 or set BOKEH_ALLOW_WS_ORIGIN=localhost:8889\nto permit this; currently we allow origins {'localhost:8888'}\n</code></pre> <p>This is related to something bokeh cannot do without explicit permission from jupyter. It can't be fixed by this library but you can circumvent this error by running jupyter via;</p> <pre><code>python -m jupyter lab --port 8889 --allow-websocket-origin=localhost:8889\n</code></pre> <p>You can also set an environment variable <code>BOKEH_ALLOW_WS_ORIGIN=localhost:8889</code>.</p>"},{"location":"examples/model-mining/","title":"Model Mining","text":"<p>In this example, we will demonstrate that you can use visual data mining techniques to discover meaningful patterns in your data. These patterns can be easily translated into a machine learning model by using the tools found in this package. </p> <p>You can find a full tutorial of this technique on calmcode but the main video can be viewed below.</p>"},{"location":"examples/model-mining/#the-task","title":"The Task","text":"<p>We're going to make a rule based model for the creditcard dataset. The main feature of the dataset is that it is suffering from a class imbalance. Instead of training a machine learning model, let's try to instead explore it with a parallel coordinates chart. If you scroll all the way to the bottom of this tutorial you'll see an example of such a chart. It shows a \"train\"-set.</p> <p>We explored the data just like in the video and that led us to define the following model. </p> <pre><code>from hulearn.classification import FunctionClassifier\nfrom hulearn.experimental import CaseWhenRuler\n\ndef make_prediction(dataf, age=15):\n    ruler = CaseWhenRuler(default=0)\n\n    (ruler\n     .add_rule(lambda d: (d['V11'] &gt; 4), 1)\n     .add_rule(lambda d: (d['V17'] &lt; -3), 1)\n     .add_rule(lambda d: (d['V14'] &lt; -8), 1))\n\n    return ruler.predict(dataf)\n\nclf = FunctionClassifier(make_prediction)\n</code></pre> Full Code <p>First we load the data.</p> <p>```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split</p> <p>df_credit = fetch_openml(     data_id=1597,     as_frame=True )</p> <p>credit_train, credit_test = train_test_split(df_credit, test_size=0.5, shuffle=True) ```</p> <p>Next, we create a hiplot in jupyter.</p> <p>```python import json  import hiplot as hip</p> <p>samples = [credit_train.loc[lambda d: d['group'] == True], credit_train.sample(5000)] json_data = pd.concat(samples).to_json(orient='records')</p> <p>hip.Experiment.from_iterable(json.loads(json_data)).display() ```</p> <p>Given that we have our model, we can make a classification report. </p> <p>```python from sklearn.metrics import classification_report</p> <p>When we ran the benchmark locally, we got the following classification report.</p> <pre><code>              precision    recall  f1-score   support\n\n       False       1.00      1.00      1.00    142165\n        True       0.70      0.73      0.71       239\n\n    accuracy                           1.00    142404\n   macro avg       0.85      0.86      0.86    142404\nweighted avg       1.00      1.00      1.00    142404\n</code></pre>"},{"location":"examples/model-mining/#note-that-fit-is-a-no-op-here","title":"Note that <code>fit</code> is a no-op here.","text":"<p>preds = clf.fit(credit_train, credit_train['group']).predict(credit_test)) print(classification_report(credit_test['group'], preds) ```</p>"},{"location":"examples/model-mining/#deep-learning","title":"Deep Learning","text":"<p>It's not a perfect benchmark, but we could compare this result to the one that's demonstrated  on the keras blog. The  trained model there lists 86.67% precision but only 23.9% recall. Depending on your preferences for false-positives, you could argue that our model is outperforming the deep learning model. </p> <p>It's not 100% a fair comparison. You can imagine that the keras blogpost is written to explain  keras. The auther likely didn't attempt to make a state-of-the-art model. But what this demo does show is the merit of turning an exploratory data analysis into a model. You can end up with a  very interpretable model, you might learn something about your data along the way and the model might certainly still perform well.</p>"},{"location":"examples/model-mining/#parallel-coordinates","title":"Parallel Coordinates","text":"<p>If you hover of the <code>group</code> name and right-click, you'll be able to set it for coloring and  repeat the experience in the video. By doing that it becomes quite easy to eyeball how to separate the two classes. The <code>V17</code> column especially seems powerful here. In real life we might ask \"why?\" this column is so distinctive but for now we'll just play around until we find a sensible model. </p>"},{"location":"examples/usage/","title":"S\u1eed d\u1ee5ng","text":"<p>This page contains a list of short examples that demonstrate the utility of the tools in this package. The goal for each example is to be small and consise.</p>"},{"location":"examples/usage/#precision-and-subgroups","title":"Precision and Subgroups","text":"<p>It can be the case that for a subgroup of the population you do not need a model. Suppose that we have a session log dataset from \"World of Warcraft\". We know when people logged in, if they were part of a guild and when they stopped playing. You can create a machine learning model to predict which players are at risk of quitting the game but you might also be able to come up with some simple rules.</p> <p>Here is one rule that might work out swell:</p> <p>\"If any player was playing the video game at 24:00 on new-years eve, odds are that this person is very invested in the game and won't stop playing.\"</p> <p>This one rule will not cover the entire population but for the subgroup it can be an effective rule.</p> <p></p> <p>As an illustrative example we'll implement this diagram as a <code>Classifier</code>.</p> <pre><code>import numpy as np\nfrom hulearn.outlier import InteractiveOutlierDetector\nfrom hulearn.classification import FunctionClassifier, InteractiveClassifier\n\n\nclassifier = SomeScikitLearnModel()\n\ndef make_decision(dataf):\n    # First we create a resulting array with all the predictions\n    res = classifier.predict(dataf)\n\n    # Override model prediction if a user is a heavy_user, no matter what\n    res = np.where(dataf['heavy_user'], \"stays\", res)\n\n    return res\n\nfallback_model = FunctionClassifier(make_decision)\n</code></pre>"},{"location":"examples/usage/#no-data-no-problem","title":"No Data No Problem","text":"<p>Let's say that we're interested in detecting fraud at a tax office. Even without looking at the data we can already come up with some sensible rules.</p> <ul> <li>Any minor making over the median income is \"suspicious\".</li> <li>Any person who started more than 2 companies in a year is \"suspicious\".</li> <li>Any person who has more than 10 bank accounts is \"suspicious\".</li> </ul> <p>The thing with these rules is that they are easy to explain but they are not based on data at all. In fact, they may not occur in the data at all. This means that a machine learning model may not have picked up this pattern that we're interested in. Thankfully, the lack in data can be compensated with business rules.</p> <p></p>"},{"location":"examples/usage/#comfort-zone","title":"Comfort Zone","text":"<p>Models typically have a \"comfort zone\". If a new data point comes in that is very different from what the models saw before it should not be treated the same way. You can also argue that points with low <code>proba</code> score should also not be automated.</p> <p>If you want to prevent predictions where the model is \"unsure\" then you might want to follow this diagram;</p> <p></p> <p>You can construct such a system by creating a <code>FunctionClassifier</code> that handles the logic you require. As an illustrative example we'll implement this diagram as a <code>Classifier</code>.</p> <pre><code>import numpy as np\nfrom hulearn.outlier import InteractiveOutlierDetector\nfrom hulearn.classification import FunctionClassifier, InteractiveClassifier\n\n# We're importing a classifier/outlier detector from our library\n# but nothing is stopping you from using those in scikit-learn.\n# Just make sure that they are trained beforehand!\noutlier    = InteractiveOutlierDetector.from_json(\"path/to/file.json\")\nclassifier = InteractiveClassifier.from_json(\"path/to/file.json\")\n\ndef make_decision(dataf):\n    # First we create a resulting array with all the predictions\n    res = classifier.predict(dataf)\n\n    # If we detect doubt, \"classify\" it as a fallback instead.\n    proba = classifier.predict_proba(dataf)\n    res = np.where(proba.max(axis=1) &lt; 0.8, \"doubt_fallback\", res)\n\n    # If we detect an ourier, we'll fallback too.\n    res = np.where(outlier.predict(dataf) == -1, \"outlier_fallback\", res)\n\n    # This `res` array contains the output of the drawn diagram.\n    return res\n\nfallback_model = FunctionClassifier(make_decision)\n</code></pre> <p>For more information on why this tactic is helpful:</p> <ul> <li>blogpost</li> <li>pydata talk</li> </ul>"},{"location":"guide/drawing-classifier/drawing/","title":"Drawing as a Model","text":""},{"location":"guide/drawing-classifier/drawing/#classic-classification-problem","title":"Classic Classification Problem","text":"<p>Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here.</p> <p></p> <p>The goal is to predict the colors of the points. Very commonly folks would look at this and say;</p> <p></p> <p>But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier.</p>"},{"location":"guide/drawing-classifier/drawing/#lets-draw","title":"Let's Draw!","text":"<p>Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas.</p>"},{"location":"guide/drawing-classifier/drawing/#instructions","title":"Instructions","text":"<p>To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace.</p> <p>Once you're done drawing you might end up with a drawing that looks like this.</p> <p></p> <p>When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model?</p>"},{"location":"guide/drawing-classifier/drawing/#properties-of-modelling-technique","title":"Properties of Modelling Technique","text":"<p>Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider.</p> <ol> <li>By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis.</li> <li>By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity.</li> <li>You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it!</li> <li>You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful.</li> <li>We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism.</li> <li>If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model.</li> </ol>"},{"location":"guide/drawing-classifier/drawing/#from-jupyter","title":"From Jupyter","text":"<p>In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works.</p> <pre><code>from sklego.datasets import load_penguins\nfrom hulearn.experimental.interactive import InteractiveCharts\n\ndf = load_penguins(as_frame=True).dropna()\nclf = InteractiveCharts(df, labels=\"species\")\n</code></pre> <p>The <code>clf</code> variable contains a <code>InteractiveCharts</code> object that has assumed that the <code>\"species\"</code> column in <code>df</code> to represent the label that we're interested in. From here you can generate charts, via;</p> <pre><code># It's best to run this in a single cell.\nclf.add_chart(x=\"bill_length_mm\", y=\"bill_depth_mm\")\n</code></pre> <p>You can also generate a second chart.</p> <pre><code># Again, run this in a seperate cell.\nclf.add_chart(x=\"flipper_length_mm\", y=\"body_mass_g\")\n</code></pre> <p>This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this;</p> <p></p>"},{"location":"guide/drawing-classifier/drawing/#serialize","title":"Serialize","text":"<p>You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json.</p> <pre><code>drawn_data = clf.data()\n# You can also save the data to disk if you want.\nclf.to_json(\"drawn-model.json\")\n</code></pre> What the json file looks like. <pre><code>[{'chart_id': '3c680a70-0',\n'x': 'bill_length_mm',\n'y': 'bill_depth_mm',\n'polygons': {'Adelie': {'bill_length_mm': [[32.14132787891895,\n32.84074984423687,\n38.78583654943918,\n46.829189150595255,\n47.17890013325422,\n43.68179030666462,\n35.63843770550855]],\n'bill_depth_mm': [[15.406862190509665,\n19.177207018095874,\n21.487207018095873,\n21.5934139146476,\n19.217943123601575,\n16.640631196069247,\n15.244587235322568]]},\n'Gentoo': {'bill_length_mm': [[58.10736834134671,\n50.501154468514336,\n40.18468048007502,\n40.09725273441028,\n44.556067763312015,\n53.12398683845653,\n58.894218052329364,\n60.76142402357685]],\n'bill_depth_mm': [[17.284959177952327,\n17.553429170403614,\n14.627106252684614,\n13.201081726611287,\n12.051605398390103,\n13.827533449580619,\n15.667347786949287,\n17.024587871893388]]},\n'Chinstrap': {'bill_length_mm': [[44.11892903498832,\n40.88410244539294,\n45.51777296562416,\n51.72514290782069,\n56.621096665046124,\n58.019940595681966,\n53.29884232978601,\n52.511992618803355,\n47.004044641924736]],\n'bill_depth_mm': [[16.103691211166677,\n16.72117219380463,\n19.217943123601575,\n20.85561007755441,\n21.124080070005693,\n19.540107114543115,\n18.57361514171849,\n16.39900820286309,\n15.915762216450778]]}}},\n{'chart_id': '198b23fb-5',\n'x': 'flipper_length_mm',\n'y': 'body_mass_g',\n'polygons': {'Adelie': {'flipper_length_mm': [[205.39985750238202,\n205.39985750238202,\n184.0772104628077,\n174.80649435864495,\n170.235872105095,\n161.6171609214579,\n174.42229536301556,\n194.38496200094178,\n197.57898866300997,\n209.5565886457657,\n204.4993797641577]],\n'body_mass_g': [[4079.2264346061725,\n4876.092877056334,\n4876.092877056334,\n4067.842628285456,\n3521.4199248910595,\n3088.8352847038286,\n2781.4725140444807,\n2781.4725140444807,\n3134.370509986695,\n3555.5713438532093,\n3737.7122449846747]]},\n'Gentoo': {'flipper_length_mm': [[208.77192413146238,\n201.53909280616116,\n216.39571931218526,\n232.7342009323645,\n241.9517683831975,\n222.55068229508308]],\n'body_mass_g': [[3898.03455221242,\n4740.103517729661,\n6171.487627453468,\n6230.793172902075,\n5650.448345315868,\n4603.5517935917305]]},\n'Chinstrap': {'flipper_length_mm': [[215.1341094117529,\n195.202069787803,\n173.41588694302055,\n181.06422772895482,\n197.75151671644775,\n212.35289458050406]],\n'body_mass_g': [[4330.448345315868,\n4626.310414281385,\n3272.1724832469026,\n2698.5776834613475,\n2872.5429646102134,\n3646.641794418942]]}}}]\n</code></pre> <p>This data represents the drawings that you've made.</p>"},{"location":"guide/drawing-classifier/drawing/#model","title":"Model","text":"<p>This generated data can be read in by our <code>InteractiveClassifier</code> which will allow you to use your drawn model as a scikit-learn model.</p> <pre><code>from hulearn.classification import InteractiveClassifier\n\nmodel = InteractiveClassifier(json_desc=drawn_data)\n# Alternatively you can also load from disk.\nInteractiveClassifier.from_json(\"drawn-model.json\")\n</code></pre> <p>This model can be used to make predictions but you will still need to follow the standard <code>.fit(X, y)</code> and <code>.predict(X)</code> pattern.</p> <pre><code>X, y = df.drop(columns=['species']), df['species']\n\npreds = model.fit(X, y).predict_proba(X)\n</code></pre> <p>We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values <code>preds</code> plotted over the original charts that we drew.</p> <p> </p> Code for the plots. <pre><code>import matplotlib.pylab as plt\n\nplt.figure(figsize=(12, 3))\nfor i in range(3):\n    plt.subplot(131 + i)\n    plt.scatter(X['bill_length_mm'], X['bill_depth_mm'], c=preds[:, i])\n    plt.xlabel('bill_length_mm')\n    plt.ylabel('bill_depth_mm')\n    plt.title(model.classes_[i])\n\nimport matplotlib.pylab as plt\n\nplt.figure(figsize=(12, 3))\nfor i in range(3):\n    plt.subplot(131 + i)\n    plt.scatter(X['flipper_length_mm'], X['body_mass_g'], c=preds[:, i])\n    plt.xlabel('flipper_length_mm')\n    plt.ylabel('body_mass_g')\n    plt.title(model.classes_[i])\n</code></pre> <p>Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it.</p> <p>The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API.</p>"},{"location":"guide/drawing-classifier/drawing/#conclusion","title":"Conclusion","text":"<p>The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules.</p> <p>Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems.</p>"},{"location":"guide/drawing-classifier/drawing/#notebook","title":"Notebook","text":"<p>If you want to run this code yourself, feel free to download the notebook.</p>"},{"location":"guide/drawing-features/custom-features/","title":"Drawing Features VN","text":"<p>Sofar we've explored drawing as a tool for models, but it can also be used as a tool to generate features. To explore this, let's load in the penguins dataset again.</p> <pre><code>from sklego.datasets import load_penguins\n\ndf = load_penguins(as_frame=True).dropna()\n</code></pre>"},{"location":"guide/drawing-features/custom-features/#drawing","title":"Drawing","text":"<p>We can draw over this dataset. It's like before but with one crucial differenc</p> <pre><code>from hulearn.experimental.interactive import InteractiveCharts\n\n# Note that the `labels` arugment here is a list, not a string! This\n# tells the tool that we want to be able to add custom groups that are\n# not defined by a column in the dataframe.\ncharts = InteractiveCharts(df, labels=['group_one', 'group_two'])\n</code></pre> <p>Let's make a custom drawing.</p> <pre><code>charts.add_chart(x=\"flipper_length_mm\", y=\"body_mass_g\")\n</code></pre> <p>Let's assume the new drawing looks something like this.</p> <p></p> <p>Sofar these drawn features have been used to construct models. But they can also be used to help label data or generate extra features for machine learning models.</p>"},{"location":"guide/drawing-features/custom-features/#features","title":"Features","text":"<p>This library makes it easy to add these features to scikit-learn pipelines or to pandas. To get started, you'll want to import the <code>InteractivePreprocessor</code>.</p> <pre><code>from hulearn.preprocessing import InteractivePreprocessor\ntfm = InteractivePreprocessor(json_desc=charts.data())\n</code></pre> <p>This <code>tfm</code> object is can be used as a preprocessing step inside of scikit-learn but it can also be used in a pandas pipeline.</p> <pre><code># The flow for scikit-learn\ntfm.fit(df).transform(df)\n# The flow for pandas\ndf.pipe(tfm.pandas_pipe)\n</code></pre>"},{"location":"guide/finding-outliers/outliers/","title":"Outliers and Comfort VN","text":""},{"location":"guide/finding-outliers/outliers/#rethinking-classification","title":"Rethinking Classification","text":"<p>Let's have another look at the interactive canvas that we saw in the previous guide.</p> <pre><code>from sklego.datasets import load_penguins\nfrom hulearn.experimental.interactive import InteractiveCharts\n\ndf = load_penguins(as_frame=True).dropna()\nclf = InteractiveCharts(df, labels=\"species\")\n</code></pre> <p></p> <p>The drawn colors indicate that a human deemed a classification appropriate. You could wonder what we might want to do with the regions that have not been colored though. Machine learning algorithms might typically still assign a class to those regions but that can be a dangerous idea.</p> <p></p> <p>We might consider these points outside of the \"comfort zone\" of the predicted areas. In these situations it might be best to declare it an outlier and to handle it differently. That way we don't automate a decision that we're likely to regret later.</p>"},{"location":"guide/finding-outliers/outliers/#outliers","title":"Outliers","text":"<p>The drawn charts can be used to construct a classifier but they may also be used to construct an outlier detection model. This allows us to re-use earlier work for multiple purposes.</p> <pre><code>from sklego.datasets import load_penguins\nfrom hulearn.experimental.interactive import InteractiveCharts\n\ndf = load_penguins(as_frame=True).dropna()\n\ncharts = InteractiveCharts(df, labels=\"species\")\n\n# Run this in a seperate cell\ncharts.add_chart(x=\"bill_length_mm\", y=\"bill_depth_mm\")\n\n# Run this in a seperate cell\ncharts.add_chart(x=\"flipper_length_mm\", y=\"body_mass_g\")\n</code></pre> <p>To demonstrate how it works, let's assume that we've drawn the following:</p> <p></p> <p>We'll again fetch the drawn data but now we'll use it to detect outliers.</p> <pre><code>from hulearn.outlier import InteractiveOutlierDetector\n\n# Load the model using drawn-data.\nmodel = InteractiveOutlierDetector(json_desc=charts.data())\n\nX, y = df.drop(columns=['species']), df['species']\npreds = model.fit(X, y).predict(X)\n</code></pre> <p>This model can now be used as a scikit-learn compatible outlier detection model. Here's the output of the model.</p> <p></p> Code for the plots.     ```python     import matplotlib.pylab as plt      plt.figure(figsize=(10, 4))     plt.subplot(121)     plt.scatter(X['bill_length_mm'], X['bill_depth_mm'], c=preds)     plt.xlabel('bill_length_mm')     plt.ylabel('bill_depth_mm')     plt.subplot(122)     plt.scatter(X['flipper_length_mm'], X['body_mass_g'], c=preds)     plt.xlabel('flipper_length_mm')     plt.ylabel('body_mass_g');     ```"},{"location":"guide/finding-outliers/outliers/#how-it-works","title":"How it works.","text":"<p>A point is considered an outlier if it does not fall inside of enough drawn polygons. The number of poylgons that a point must fall into is a parameter that you can set manually or even search for in a grid-search. For example, let's repeat the exercise. The base setting is that a point needs to be in at least one polygon but we can change this to two.</p> <pre><code># Before\nmodel = InteractiveOutlierDetector(json_desc=charts.data(), threshold=1)\n# After\nmodel = InteractiveOutlierDetector(json_desc=charts.data(), threshold=2)\n</code></pre> <p></p>"},{"location":"guide/finding-outliers/outliers/#combine","title":"Combine","text":"<p>You might wonder, can we combine the <code>FunctionClassifier</code> with an outlier model like we've got here? Yes! Use a <code>FunctionClassifier</code>!</p> <p></p> <p>As an illustrative example we'll implement a diagram like above as a <code>Classifier</code>.</p> <pre><code>import numpy as np\nfrom hulearn.outlier import InteractiveOutlierDetector\nfrom hulearn.classification import FunctionClassifier, InteractiveClassifier\n\noutlier    = InteractiveOutlierDetector.from_json(\"path/to/file.json\")\nclassifier = InteractiveClassifier.from_json(\"path/to/file.json\")\n\ndef make_decision(dataf):\n    # First we create a resulting array with all the predictions\n    res = classifier.predict(dataf)\n\n    # If we detect doubt, \"classify\" it as a fallback instead.\n    proba = classifier.predict_proba(dataf)\n    res = np.where(proba.max(axis=1) &lt; 0.8, \"doubt_fallback\", res)\n\n    # If we detect an ourier, we'll fallback too.\n    res = np.where(outlier.predict(dataf) == -1, \"outlier_fallback\", res)\n\n    # This `res` array contains the output of the drawn diagram.\n    return res\n\nfallback_model = FunctionClassifier(make_decision)\n</code></pre>"},{"location":"guide/function-classifier/function-classifier/","title":"Function as a Model VN","text":"<p>The goal of this library is to make it easier to declare common sense models. A very pythonic way of getting there is to declare a function. One of the first features in this library is the ability to re-use functions as if they are scikit-learn models.</p>"},{"location":"guide/function-classifier/function-classifier/#titanic","title":"Titanic","text":"<p>Let's see how this might work. We'll grab a dataset that is packaged along with this library.</p> <pre><code>from hulearn.datasets import load_titanic\n\ndf = load_titanic(as_frame=True)\ndf.head()\n</code></pre> <p>The <code>df</code> variable represents a dataframe and it has the following contents:</p> survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 <p>There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe.</p>"},{"location":"guide/function-classifier/function-classifier/#preparation","title":"Preparation","text":"<p>To prepare our data we will first get it into the common <code>X</code>, <code>y</code> format for scikit-learn.</p> <pre><code>X, y = df.drop(columns=['survived']), df['survived']\n</code></pre> <p>We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense?</p> <p>It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better.</p>"},{"location":"guide/function-classifier/function-classifier/#functionclassifier","title":"FunctionClassifier","text":"<p>Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the <code>FunctionClassifier</code>. You can see an example of that below.</p> <pre><code>import numpy as np\nfrom hulearn.classification import FunctionClassifier\n\ndef fare_based(dataf, threshold=10):\n\"\"\"\n    The assumption is that folks who paid more are wealthier and are more\n    likely to have recieved access to lifeboats.\n    \"\"\"\n    return np.array(dataf['fare'] &gt; threshold).astype(int)\n\nmod = FunctionClassifier(fare_based)\n</code></pre> <p>This <code>mod</code> is a scikit-learn model, which means that you can <code>.fit(X, y).predict(X)</code>.</p> <pre><code>mod.fit(X, y).predict(X)\n</code></pre> <p>During the <code>.fit(X, y)</code>-step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step.</p>"},{"location":"guide/function-classifier/function-classifier/#grid","title":"Grid","text":"<p>Being able to <code>.fit(X, y).predict(X)</code> is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best <code>threshold</code> value? For that, you might like to use <code>GridSearchCV</code>.</p> <pre><code>from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, make_scorer\n\n# Note the threshold keyword argument in this function.\ndef fare_based(dataf, threshold=10):\n    return np.array(dataf['fare'] &gt; threshold).astype(int)\n\n# Pay attention here, we set the threshold argument in here.\nmod = FunctionClassifier(fare_based, threshold=10)\n\n# The GridSearch object can now \"grid-search\" over this argument.\n# We also add a bunch of metrics to our approach so we can measure.\ngrid = GridSearchCV(mod,\n                    cv=2,\n                    param_grid={'threshold': np.linspace(0, 100, 30)},\n                    scoring={'accuracy': make_scorer(accuracy_score),\n                             'precision': make_scorer(precision_score),\n                             'recall': make_scorer(recall_score)},\n                    refit='accuracy')\ngrid.fit(X, y)\n</code></pre> <p>If we make a chart of the <code>grid.cv_results_</code> then they would look something like;</p> <p></p> <p>A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a <code>RandomForestClassifier</code> using the <code>'pclass', 'sex', 'age', 'fare'</code> columns that the precision score would be about the same.</p>"},{"location":"guide/function-classifier/function-classifier/#bigger-grids","title":"Bigger Grids","text":"<p>You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that.</p> <pre><code>def last_name(dataf, sex='male', pclass=1):\n    predicate = (dataf['sex'] == sex) &amp; (dataf['pclass'] == pclass)\n    return np.array(predicate).astype(int)\n\n# Once again, remember to declare your arguments here too!\nmod = FunctionClassifier(last_name, pclass=10, sex='male')\n\n# The arguments of the function can now be \"grid-searched\".\ngrid = GridSearchCV(mod,\n                    cv=2,\n                    param_grid={'pclass': [1, 2, 3], 'sex': ['male', 'female']},\n                    scoring={'accuracy': make_scorer(accuracy_score),\n                             'precision': make_scorer(precision_score),\n                             'recall': make_scorer(recall_score)},\n                    refit='accuracy')\ngrid.fit(X, y)\n</code></pre>"},{"location":"guide/function-classifier/function-classifier/#guidance","title":"Guidance","text":"<p>Human Learn doesn't just allow you to turn functions into classifiers. It also tries to help you find rules that could be useful. In particular, an interactive parallel coordinates chart could be very helpful here.</p> <p>You can create a parallel coordinates chart directly inside of jupyter.</p> <pre><code>from hulearn.experimental.interactive import parallel_coordinates\nparallel_coordinates(df, label=\"survived\", height=200)\n</code></pre> <p>What follows next are some explorations of the dataset. They are based on the scene from the titanic movie where they yell \"Woman and Children First!\". So let's see if we can confirm if this holds true.</p>"},{"location":"guide/function-classifier/function-classifier/#explore","title":"Explore","text":"<p>It indeed seems that women in 1st/2nd class have a high chance of surviving.</p> <p></p> <p>It also seems that male children have an increased change of survival, but only if they were travelling 1st/2nd class.</p>"},{"location":"guide/function-classifier/function-classifier/#grid_1","title":"Grid","text":"<p>Here's a lovely observation. By doing exploratory analysis we not only understand the data better but we can now also turn the patterns that we've observed into a model!</p> <pre><code>def make_prediction(dataf, age=15):\n    women_rule = (dataf['pclass'] &lt; 3.0) &amp; (dataf['sex'] == \"female\")\n    children_rule = (dataf['pclass'] &lt; 3.0) &amp; (dataf['age'] &lt;= age)\n    return women_rule | children_rule\n\nmod = FunctionClassifier(make_prediction)\n</code></pre> <p>We're even able to use grid-search again to find the optimal threshold for <code>\"age\"</code>.</p>"},{"location":"guide/function-classifier/function-classifier/#comparison","title":"Comparison","text":"<p>To compare our results we've also trained a <code>RandomForestClassifier</code>. Here's how the models compare;</p> Model accuracy precision recall Women &amp; Children Rule 0.808157 0.952168 0.558621 RandomForestClassifier 0.813869 0.785059 0.751724 <p>It seems like our rule based model is quite reasonable. A great follow-up exercise would be to try and understand when the random forest model disagrees with the rule based system. This could lead us to understand more patterns in the data.</p>"},{"location":"guide/function-classifier/function-classifier/#conclusion","title":"Conclusion","text":"<p>In this guide we've seen the <code>FunctionClassifier</code> in action. It is one of the many models in this  library that will help you construct more \"human\" models. This component is very effective when it is combined with exploratory data analysis techniques.</p>"},{"location":"guide/function-classifier/function-classifier/#notebook","title":"Notebook","text":"<p>If you want to download with this code yourself, feel free to download the notebook here.</p>"},{"location":"guide/function-preprocess/function-preprocessing/","title":"Human Preprocessing VN","text":"<p>In python the most popular data analysis tool is pandas while the most popular tool for making models is scikit-learn. We love the data wrangling tools of pandas while we appreciate the benchmarking capability of scikit-learn.</p> <p>The fact that these tools don't fully interact is slightly awkward. The data going into the model has an big effect on the output.</p> <p>So how might we more easily combine the two?</p>"},{"location":"guide/function-preprocess/function-preprocessing/#pipe","title":"Pipe","text":"<p>In pandas there's an amazing trick that you can do with the <code>.pipe</code> method. We'll give a quick overview on how it works but if you're new to this idea you may appreciate this resource or this blogpost.</p> <pre><code>from hulearn.datasets import load_titanic\n\ndf = load_titanic(as_frame=True)\nX, y = df.drop(columns=['survived']), df['survived']\nX.head(4)\n</code></pre> <p>The goal of the titanic dataset is to predict weather or not a passenger survived the disaster. The <code>X</code> variable represents a dataframe with variables that we're going to use to predict survival (stored in <code>y</code>). Here's a preview of what <code>X</code> might have.</p> pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 <p>Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code.</p> <pre><code>X['nchar'] = X['name'].str.len()\n</code></pre> <p>This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function.</p> <pre><code>def process(dataf):\n    # Make a copy of the dataframe to prevent it from overwriting the original data.\n    dataf = dataf.copy()\n    # Make the changes\n    dataf['nchar'] = dataf['name'].str.len()\n    # Return the name dataframe\n    return dataf\n</code></pre> <p>We now have a nice function that makes our changes and we can use it like so;</p> <pre><code>X_new = process(X)\n</code></pre> <p>We can do something more powerful though.</p>"},{"location":"guide/function-preprocess/function-preprocessing/#paramaters","title":"Paramaters","text":"<p>Let's make some more changes to our <code>process</code> function.</p> <pre><code>def preprocessing(dataf, n_char=True, gender=True):\n    dataf = dataf.copy()\n    if n_char:\n        dataf['nchar'] = dataf['name'].str.len()\n    if gender:\n        dataf['gender'] = (dataf['sex'] == 'male').astype(\"float\")\n    return dataf.drop(columns=[\"name\", \"sex\"])\n</code></pre> <p>This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end.</p> <p>We've changed the way we've defined our function but we're also changing the way that we're going to apply it.</p> <pre><code># This is equivalent to preprocessing(X)\nX.pipe(preprocessing)\n</code></pre> <p>The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview.</p>"},{"location":"guide/function-preprocess/function-preprocessing/#with-pipe","title":"With <code>.pipe()</code>","text":"<pre><code>(df\n  .pipe(set_col_types)\n  .pipe(preprocessing, nchar=True, gender=False)\n  .pipe(add_time_info))\n</code></pre>"},{"location":"guide/function-preprocess/function-preprocessing/#without-pipe","title":"Without <code>.pipe()</code>","text":"<pre><code>add_time_info(preprocessing(set_col_types(df), nchar=True, gender=False))\n</code></pre> <p>Let's be honest, this looks messy.</p>"},{"location":"guide/function-preprocess/function-preprocessing/#pipetransformer","title":"PipeTransformer","text":"<p>It would be great if we could use the <code>preprocessing</code>-function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas <code>.pipe</code>-line in general!</p> <p>For that we've got another feature in our library, the <code>PipeTransformer</code>.</p> <pre><code>from hulearn.preprocessing import PipeTransformer\n\ndef preprocessing(dataf, n_char=True, gender=True):\n    dataf = dataf.copy()\n    if n_char:\n        dataf['nchar'] = dataf['name'].str.len()\n    if gender:\n        dataf['gender'] = (dataf['sex'] == 'male').astype(\"float\")\n    return dataf.drop(columns=[\"name\", \"sex\"])\n\n# Important, don't forget to declare `n_char` and `gender` here.\ntfm = PipeTransformer(preprocessing, n_char=True, gender=True))\n</code></pre> <p>The <code>tfm</code> variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function.</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\n\n\npipe = Pipeline([\n    ('prep', tfm),\n    ('mod', GaussianNB())\n])\n\nparams = {\n    \"prep__n_char\": [True, False],\n    \"prep__gender\": [True, False]\n}\n\ngrid = GridSearchCV(pipe, cv=3, param_grid=params).fit(X, y)\n</code></pre> <p>Once trained we can fetch the <code>grid.cv_results_</code> to get a glimpse at the results of our pipeline.</p> param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 <p>It seems that we gender of the passenger has more of an effect on their survival than the length of their name.</p>"},{"location":"guide/function-preprocess/function-preprocessing/#utility","title":"Utility","text":"<p>The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code.</p> <p>There's a few small caveats to be aware of.</p>"},{"location":"guide/function-preprocess/function-preprocessing/#dont-remove-data","title":"Don't remove data","text":"<p>Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this.</p>"},{"location":"guide/function-preprocess/function-preprocessing/#dont-sort-data","title":"Don't sort data","text":"<p>You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the <code>y</code> variable that you're trying to predict.</p>"},{"location":"guide/function-preprocess/function-preprocessing/#dont-use-lambda","title":"Don't use <code>lambda</code>","text":"<p>There's two ways that you can add a new column to pandas.</p> <pre><code># Method 1\ndataf_new = dataf.copy() # Don't overwrite data!\ndataf_new['new_column'] = dataf_new['old_column'] * 2\n\n# Method 2\ndataf_new = dataf.assign(lambda d: d['old_column'] * 2)\n</code></pre> <p>In many cases you might argue that method #2 is safer because you do not need to worry about the <code>dataf.copy()</code> that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use <code>lambda</code> functions because it cannot pickle the code.</p>"},{"location":"guide/function-preprocess/function-preprocessing/#dont-cheat","title":"Don't Cheat!","text":"<p>The functions that you write are supposed to be stateless in the sense that they don't learn from the data that goes in. You could theoretically bypass this with global variables but by doing so you're doing yourself a disservice. If you do this you'll be cheating the statistics by leaking information.</p>"},{"location":"problemsolving/Test_CrackItSummary/","title":"Cracked it! How to solve big problems and sell solutions like top strategy consultants &gt;&gt;","text":""},{"location":"problemsolving/Test_CrackItSummary/#chapter-1-the-most-important-skill-you-never-learned","title":"Chapter 1: The Most Important Skill You Never Learned &gt;&gt;","text":""},{"location":"problemsolving/Test_CrackItSummary/#1-fast-and-slow-problem-solving","title":"1. Fast and Slow Problem Solving &gt;&gt;","text":""},{"location":"problemsolving/Test_CrackItSummary/#chung-ta-thuong-co-xu-huong-ua-ra-giai-phap-nhanh-chong-va-on-gian-hoa-1-van-e-phuc-tap-khi-chua-hieu-ro-nhung-lai-co-niem-tin-sai-lech-rang-chung-ta-a-biet-moi-thu-u-ro-e-giai-quyet-c-van-e","title":"Ch\u00fang ta th\u01b0\u1eddng c\u00f3 xu h\u01b0\u1edbng \u0111\u01b0a ra gi\u1ea3i ph\u00e1p nhanh ch\u00f3ng v\u00e0 \u0111\u01a1n gi\u1ea3n h\u00f3a 1 v\u1ea5n \u0111\u1ec1 ph\u1ee9c t\u1ea1p khi ch\u01b0a hi\u1ec3u r\u00f5, nh\u01b0ng l\u1ea1i c\u00f3 ni\u1ec1m tin sai l\u1ec7ch r\u1eb1ng ch\u00fang ta \u0111\u00e3 bi\u1ebft m\u1ecdi th\u1ee9 \u0111\u1ee7 r\u00f5 \u0111\u1ec3 gi\u1ea3i quy\u1ebft \u0111c v\u1ea5n \u0111\u1ec1 &gt;&gt;","text":"<ul> <li>At the time of Kelvin Rollins\u2019s anointing as Dell\u2019s CEO, Dell was the world\u2019s largest and most profitable producer of computers. Only two and a half years later, at the beginning of 2007, the situation was very different &gt;&gt;</li> <li>The problem Dell faced in early 2007 involved a complex set of poorly understood factors, which made it hard to define the problem, let alone know how to solve it. &gt;&gt;</li> <li>Such complex and ill-defined problems are idiosyncratic and infrequently occur, making it difficult to develop routine solutions or approaches to solving them &gt;&gt;</li> </ul>"},{"location":"problemsolving/Test_CrackItSummary/#daniel-kahneman-psychologist-and-nobel-laureate-in-economics-explains-in-his-groundbreaking-best-seller-thinking-fast-and-slow-how-we-have-two-minds-in-one-brain","title":"Daniel Kahneman, psychologist and Nobel laureate in economics, explains in his groundbreaking best seller Thinking, Fast and Slow how we have two minds in one brain &gt;&gt;","text":"<ul> <li>System 1: Our default approach to thinking,  largely involuntary, automatic, and unconscious, \u2014including about how to solve problems\u2014is fast. When thinking fast, we limit our attention to information readily available rather than search for information that could help us better understand the situation, a tendency Kahneman calls \u201cWhat You See Is All There Is (WYSIATI).\u201d &gt;&gt;</li> <li>System 2: Slow thinking (aka \u201cSystem 2\u201d) is voluntary because it requires effortful attention and conscious deliberation. But this effort is cognitively expensive: mental capacity is a scarce resource, and we need to allocate it to the problem (thus the phrase \u201cpaying attention\u201d). Consequently, in solving challenging problems, we often gravitate toward the law of least effort. One way we do this is to rely on the results of the faster and cognitively cheaper System 1 approach to thinking. Our deliberative System 2 thinking then merely endorses System 1\u2019s proposals. &gt;&gt;<ul> <li>With sufficient effort and skill, however, slow thinking can be logical, skeptical, and methodical, causing us to search for missing information, question assumptions and beliefs, and utilize tools and frameworks to make sense of a situation, resulting in a much better understanding of it and how to tackle it &gt;&gt;</li> <li>The temptation in the story about Kevin Rollins and Dell Inc. is to think (too) fast about the problem and solution or to be lazy in our slow, deliberative thinking. &gt;&gt;</li> <li>it\u2019s easy for us to take it at face value and believe we don\u2019t need to know more. Immediately, our brains go to work on detecting the associations among the information. &gt;&gt;</li> <li>But would we come to the same conclusion if we questioned the information and searched for more? &gt;&gt;</li> <li>When we stop presuming we know what\u2019s going on, and instead question the sufficiency of the information we possess and search for more, we become more likely to overcome our assumptions and see the problem differently, enabling us to generate different and potentially better solutions &gt;&gt;</li> </ul> </li> <li>Therein lies the core problem of problem solving\u2014our tendency to think too fast (or too lazy) and jump to solutions. We spend too little time and effort understanding a problem, believing instead we know all we need. We unleash the associational machine in our minds, reflecting our implicit assumptions about causes and effects, on this limited information to develop a coherent and plausible story about what\u2019s going on and why. &gt;&gt;<ul> <li>Adam Grant explains in his book, Originals,3 people have no trouble turning any information they receive into a coherent narrative, even when the information is random. People can\u2019t help seeing signals, even in noise. &gt;&gt;</li> <li>we all run the risk to jump to conclusions and take action without questioning the implicit assumptions\u2014or the emotions\u2014that dictate the way we interpret events and information. &gt;&gt;</li> <li>The remedy is to think about problems more thoroughly, search for missing information, double-check every clue, weigh the pros and cons, and investigate all possible hypotheses &gt;&gt;</li> </ul> </li> </ul>"},{"location":"problemsolving/Test_CrackItSummary/#so-are-you-othello-or-hamlet-are-you-more-likely-to-thinkand-acttoo-fast-or-to-get-mired-in-analysis-paralysis-while-jumping-to-conclusions-and-actions-is-a-widespread-fault-in-individuals-analysis-paralysis-is-frequent-in-large-bureaucratic-organizations-that-pile-up-studies-and-reports-before-taking-any-action-or-no-action-at-all","title":"So, are you Othello or Hamlet? Are you more likely to think\u2014and act\u2014too fast, or to get mired in analysis paralysis? While jumping to conclusions and actions is a widespread fault in individuals, analysis paralysis is frequent in large, bureaucratic organizations that pile up studies and reports before taking any action or no action at all. &gt;&gt;","text":"<ul> <li>On the one hand, being fast or lazy in our thinking allows us to economize on scarce and expensive mental resources, but the resulting solutions are often poor and ineffective. &gt;&gt;</li> <li>On the other hand, slow thinking and thorough investigation are necessary to tackle complex business problems\u2014the focus of this book\u2014but the reflection process might create delays in decision-making and thwart action. &gt;&gt;</li> <li>For organizations and institutions to be both effective and efficient, they need people who can overcome these challenges to solving complex business problems &gt;&gt;<ul> <li>Conventional wisdom suggests these people should be chosen for their intelligence, experience, and expertise. But as we\u2019ll see, being smart, experienced, and well-trained may not be enough. A systematic problem- solving method is also necessary. &gt;&gt;</li> </ul> </li> <li>Problem Solving and the Expertise Trap &gt;&gt;             1. Experts have developed in-depth knowledge within a particular domain through extensive study and practice, and have mentally organized their knowledge for easy recall and use. &gt;&gt;                 1. Within their domain of expertise, experts have advantages over novices: they have more richly developed mental models of different problems and can better recognize and understand problems, often by using analogies to past problems &gt;&gt;                     1. While analogical reasoning can be a valuable source of insight and creativity, it can lead to poor solutions when problem solvers develop analogies based on superficial similarities instead of deep causal traits. When problem solvers have deep experience in a particular domain, their knowledge is salient and easy to recall, which can lead them to pay more attention to characteristics of the new setting that seem similar and ignore those that are different, and to develop superficial analogies and poor solutions. Experience can be a poor guide when working outside your area of expertise or when the nature of your work changes. &gt;&gt;                         1. Reasoning by analogy can also lead experts to develop poor solutions when faced with new but seemingly familiar situations &gt;&gt;                         1. When reasoning by analogy, a person starts with a new, unfamiliar target problem to solve. She then considers other source settings she knows well and compares them to the target through a process of similarity mapping. &gt;&gt;                         1. By finding a source problem she believes has similar characteristics as the target, she identifies a candidate solution that solved or could have solved the source problem. &gt;&gt;                         1. The whole process may be summed up like this: \u201cI\u2019ve seen something like this before, so what worked there may work here.\u201d &gt;&gt;                 1. Experts also use more effective problem-solving strategies in their areas of expertise, more carefully evaluate potential solutions against constraints, and more effectively monitor their problem-solving progress by refining solutions &gt;&gt;                 1. Even though experts are better problem solvers than novices within their areas of expertise, when they tackle problems outside their expertise or when task conditions in their fields change, they often perform like novices \u2026 or worse &gt;&gt;                     1. Experts\u2019 rich and detailed mental models can constrain their ability to understand problems and search for solutions when working outside their fields of expertise. Mental models are rigid and resistant to change, particularly when associated with successful outcomes. Experts can become trapped by their expertise. &gt;&gt;                         1. the more expertise and experience people gain, the more entrenched they become in a particular way of viewing the world &gt;&gt;                         1. Compared to novices, experts also are overconfident in their ability to understand problems outside their areas of expertise, leading them to develop worse solutions &gt;&gt;             1. Managers and consultants typically specialize in particular functional or industrial areas for much of their careers, developing expertise in these areas. &gt;&gt;         1. Complex Problems and \u201cUnknown Unknowns\u201d &gt;&gt;             1. Many business problems are complex, ill-defined, and non-routine &gt;&gt;                 1. Complex problems\u2019 many interrelated causes make them difficult to understand &gt;&gt;                 1. An ill-defined problem is one where the current situation, desired outcome, and path between the two are difficult to articulate. Complex problems are often initially ill-defined and typically non-routine &gt;&gt;                 1. A non-routine problem has idiosyncratic characteristics: we face them infrequently and lack the opportunity to develop experience and expertise in solving them &gt;&gt;                 1. The complexity of business problems often requires the integration of various domains of knowledge, exceeding the expertise of all but the polymath problem solver &gt;&gt;             1. As problem complexity increases, solvers are more likely to face \u201cunknown unknowns,\u201d further challenging the value of expertise &gt;&gt;                 1. Faced with complex problems, we rarely know the right questions to ask. &gt;&gt;                 1. The more we are unaware of the factors that produce a problem, the more likely we are to be surprised when events happen that cause our solution efforts to fail. &gt;&gt;<ol> <li>Chapter 2: The Five Pitfalls of Problem Solving &gt;&gt;</li> <li>Chapter 3: The 4S Method &gt;&gt;</li> <li>Chapter 4: State the Problem: The TOSCA Framework &gt;&gt;</li> <li>Chapter 5: Structure the Problem: Pyramids and Trees &gt;&gt;</li> <li>Chapter 6: Structure the Problem: Analytical Frameworks &gt;&gt;</li> <li>Chapter 7: Solve the Problem: Eight Degrees of Analysis &gt;&gt;</li> <li>Chapter 8: Redefine the Problem: The Design Thinking Path &gt;&gt;</li> <li>Chapter 9: Structure and Solve the Problem Using Design Thinking &gt;&gt;</li> <li>Chapter 10: Sell the Solution: Core Message and Storyline &gt;&gt;</li> <li>Chapter 11: Sell the Solution: Recommendation Report and Delivery &gt;&gt;</li> <li>Chapter 12: The 4S Method in Action &gt;&gt;</li> <li>Chapter 13: Conclusion: Becoming a Master Problem-Solver &gt;&gt;</li> </ol> </li> </ul>"},{"location":"productmanagement/productdesign/","title":"<code>from hulearn.classification import *</code>","text":"<p>::: hulearn.classification.functionclassifier</p> <p>::: hulearn.classification.interactiveclassifier</p>"},{"location":"productmanagement/productdevelopment/","title":"<code>from hulearn.classification import *</code>","text":"<p>::: hulearn.classification.functionclassifier</p> <p>::: hulearn.classification.interactiveclassifier</p>"},{"location":"productmanagement/productlaunch/","title":"<code>from hulearn.classification import *</code>","text":"<p>::: hulearn.classification.functionclassifier</p> <p>::: hulearn.classification.interactiveclassifier</p>"},{"location":"productmanagement/productstrategy/","title":"<code>from hulearn.classification import *</code>","text":"<p>::: hulearn.classification.functionclassifier</p> <p>::: hulearn.classification.interactiveclassifier</p>"},{"location":"en/#human-learn-noahnoah","title":"Human Learn NoahNoah","text":"<p>Machine Learning models should play by the rules, literally.</p>"},{"location":"en/#project-goal","title":"Project Goal","text":"<p>Back in the old days, it was common to write rule-based systems. Systems that do;</p> <p></p> <p>Nowadays, it's much more fashionable to use machine learning instead. Something like;</p> <p></p> <p>We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground but it is also capable of making bad decision. We've also reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too.</p> <p>This package contains scikit-learn compatible tools that should make it easier to construct and benchmark rule based systems that are designed by humans. You can also use it in combination with ML models.</p>"},{"location":"en/#install","title":"Install","text":"<p>You can install this tool via <code>pip</code>.</p> <pre><code>python -m pip install human-learn\n</code></pre>"},{"location":"en/#guides","title":"Guides","text":""},{"location":"en/#tutorial","title":"Tutorial","text":"<p>There is a full course on this tool available on calmcode.io. This is the first video.</p>"},{"location":"en/#getting-started","title":"Getting Started","text":"<p>To help you get started we've written some helpful getting started guides.</p> <ol> <li>Functions as a Model</li> <li>Human Preprocessing</li> <li>Drawing as a Model</li> <li>Outliers and Comfort</li> <li>Drawing Features</li> </ol> <p>You can also check out the API documentation here.</p>"},{"location":"en/#features","title":"Features","text":"<p>This library hosts a couple of models that you can play with.</p>"},{"location":"en/#interactive-drawings","title":"Interactive Drawings","text":"<p>This tool allows you to draw over your datasets. These drawings can later be converted to models or to preprocessing tools.</p> <p></p>"},{"location":"en/#classification-models","title":"Classification Models","text":""},{"location":"en/#functionclassifier","title":"FunctionClassifier","text":"<p>This allows you to define a function that can make classification predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.</p>"},{"location":"en/#interactiveclassifier","title":"InteractiveClassifier","text":"<p>This allows you to draw decision boundaries in interactive charts to create a model. You can create charts interactively in the notebook and export it as a scikit-learn compatible model.</p>"},{"location":"en/#regression-models","title":"Regression Models","text":""},{"location":"en/#functionregressor","title":"FunctionRegressor","text":"<p>This allows you to define a function that can make regression predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.</p>"},{"location":"en/#outlier-detection-models","title":"Outlier Detection Models","text":""},{"location":"en/#functionoutlierdetector","title":"FunctionOutlierDetector","text":"<p>This allows you to define a function that can declare outliers. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.</p>"},{"location":"en/#interactiveoutlierdetector","title":"InteractiveOutlierDetector","text":"<p>This allows you to draw decision boundaries in interactive charts to create a model. If a point falls outside of these boundaries we might be able to declare it an outlier. There's a threshold parameter for how strict you might want to be.</p>"},{"location":"en/#preprocessing-models","title":"Preprocessing Models","text":""},{"location":"en/#pipetransformer","title":"PipeTransformer","text":"<p>This allows you to define a function that can make handle preprocessing. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. This is especially powerful in combination with the pandas <code>.pipe</code> method. If you're unfamiliar with this amazing feature, you may appreciate this tutorial.</p>"},{"location":"en/#interactivepreprocessor","title":"InteractivePreprocessor","text":"<p>This allows you to draw features that you'd like to add to your dataset or your machine learning pipeline. You can use it via <code>tfm.fit(df).transform(df)</code> and <code>df.pipe(tfm)</code>.</p>"},{"location":"en/#datasets","title":"Datasets","text":""},{"location":"en/#titanic","title":"Titanic","text":"<p>This library hosts the popular titanic survivor dataset for demo purposes. The goal of this dataset is to predict who might have survived the titanic disaster.</p>"},{"location":"en/#fish","title":"Fish","text":"<p>The fish market dataset is also hosted in this library. The goal of this dataset is to predict the weight of fish. However, it can also be turned into a classification problem by predicting the species.</p>"},{"location":"en/examples/model-mining/","title":"Model Mining EN","text":"<p>In this example, we will demonstrate that you can use visual data mining techniques to discover meaningful patterns in your data. These patterns can be easily translated into a machine learning model by using the tools found in this package. </p> <p>You can find a full tutorial of this technique on calmcode but the main video can be viewed below.</p>"},{"location":"en/examples/usage/","title":"Usage","text":"<p>This page contains a list of short examples that demonstrate the utility of the tools in this package. The goal for each example is to be small and consise.</p>"},{"location":"en/guide/drawing-features/custom-features/","title":"Drawing Features EN","text":"<p>Sofar we've explored drawing as a tool for models, but it can also be used as a tool to generate features. To explore this, let's load in the penguins dataset again.</p> <pre><code>from sklego.datasets import load_penguins\n\ndf = load_penguins(as_frame=True).dropna()\n</code></pre>"},{"location":"en/guide/finding-outliers/outliers/","title":"Outliers and Comfort EN","text":""},{"location":"en/guide/function-classifier/function-classifier/","title":"Function as a Model EN","text":"<p>The goal of this library is to make it easier to declare common sense models. A very pythonic way of getting there is to declare a function. One of the first features in this library is the ability to re-use functions as if they are scikit-learn models.</p>"},{"location":"en/guide/function-preprocess/function-preprocessing/","title":"Human Preprocessing EN","text":"<p>In python the most popular data analysis tool is pandas while the most popular tool for making models is scikit-learn. We love the data wrangling tools of pandas while we appreciate the benchmarking capability of scikit-learn.</p> <p>The fact that these tools don't fully interact is slightly awkward. The data going into the model has an big effect on the output.</p> <p>So how might we more easily combine the two?</p>"}]}